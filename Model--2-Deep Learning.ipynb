{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://mt.mahidol.ac.th/wp-content/uploads/2019/10/OriginalLOGO.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.layers import Input, Dense,Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split,cross_validate\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import normalize,LabelEncoder\n",
    "from sklearn.metrics import r2_score,mean_squared_error\n",
    "from tensorflow.keras import Model\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from matplotlib import style\n",
    "style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For each fingerprint file--- Combining File with pChEMBL Value and Running PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Combining PCHEMPL Value for each fingerprint</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "files=os.listdir(os.path.join(os.getcwd(),'STS-dataset\\\\fingerprints'))\n",
    "\n",
    "df_STS_data=pd.read_csv(os.path.join(os.getcwd(),\"STS-dataset\\\\STS_curatedFile.csv\"))\n",
    "for dfs in files:\n",
    "    df_orginal=pd.read_csv(os.path.join(os.getcwd(),'STS-dataset\\\\fingerprints\\\\{}'.format(dfs)))##change the directory\n",
    "    df_transformed=df_orginal.merge(df_STS_data[[\"Molecule.ChEMBL.ID\",\"pChEMBL.Value\"]],how=\"left\",left_on=\"Name\",right_on=\"Molecule.ChEMBL.ID\")\n",
    "    df_transformed.drop(\"Molecule.ChEMBL.ID\",1,inplace=True)\n",
    "    if not os.path.isdir(os.path.join(os.getcwd(),'STS-dataset\\\\fingerprints\\\\transformed')):\n",
    "        os.mkdir(os.path.join(os.getcwd(),'STS-dataset\\\\fingerprints\\\\transformed'))\n",
    "    df_transformed.to_csv(os.path.join(os.getcwd(),'STS-dataset\\\\fingerprints\\\\transformed\\\\{}--transformed.csv'.format(dfs[:-4])))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>I want to combine each finger print into one dataframe therefore reducing the dimensions for each finger print through PCA after\n",
    "running the previous model-1 PCA herustic to decide optimal N for each file and then saving the reduced dimensioned dataframe for combining in next cell</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_function(df,n_compnents,visual=True):\n",
    "    Id=df[\"Name\"]\n",
    "    X=df.drop([\"pChEMBL.Value\",\"Name\"],1)\n",
    "    y=df[\"pChEMBL.Value\"]\n",
    "    pca=PCA(n_compnents)\n",
    "    X_reduced=pca.fit_transform(X)\n",
    "    ratio=pca.explained_variance_ratio_\n",
    "    sums=np.sum(ratio)\n",
    "    cummulative_sums=[]\n",
    "    sums_cums=0\n",
    "    for i in ratio:\n",
    "        sums_cums+=i\n",
    "        cummulative_sums.append(sums_cums)\n",
    "    if visual:\n",
    "        ax=plt.subplot(111)\n",
    "        x_labels=[str(i) for i in range(n_compnents)]\n",
    "        x_numeric=[i for i in range(n_compnents)]\n",
    "        ax2 = ax.twinx()\n",
    "        ax.bar(x_labels,ratio)\n",
    "        ax2.plot(x_numeric,cummulative_sums,color=\"black\")\n",
    "        ax2.set_ylabel('Cumulative Variance', color='black')\n",
    "        ax.set_title(\"Explained variance with n={}\".format(n_compnents))\n",
    "        ax.set_xlabel(\"Components\")\n",
    "        ax.set_ylabel(\"Ratio\")\n",
    "        plt.grid(False)\n",
    "    return X_reduced,ratio,Id,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_ncompnents(df,threshold):\n",
    "    '''\n",
    "    This function is used to optimize the number of components for PCA since there is infinite number of choice available.\n",
    "    The herustic I have used to optimize the N-Component is simple, I ran the PCA for n-compenents ranging from 3 to 50 and then\n",
    "    picked the variance explained by the newly added compenent. If this variance is significant enough than I kept on adding the\n",
    "    component, but if  the variance explained by the last newly added component is below threshold,it become insignificant to add the \n",
    "    new component therefore function stops running.\n",
    "    \n",
    "    Please note that threshold is an hyperparameter given by the user....\n",
    "\n",
    "    '''\n",
    "    for n in range(3,50):\n",
    "        _,ratio,_,_=PCA_function(df,n_compnents=n,visual=False)\n",
    "        if ratio[-1]<threshold:\n",
    "            return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_transformed=os.listdir(os.path.join(os.getcwd(),'STS-dataset\\\\fingerprints\\\\transformed'))##change the directory\n",
    "for dfs in files_transformed:\n",
    "    df=pd.read_csv(os.path.join(os.getcwd(),'STS-dataset\\\\fingerprints\\\\transformed\\\\{}.csv'.format(dfs[:-4])))\n",
    "    n=best_ncompnents(df,0.001)\n",
    "    X_reduced,ratio,Id,y=PCA_function(df,n,visual=False)\n",
    "    df_transformed_reduced=pd.DataFrame(X_reduced,columns=[str(i) for i in range(1,n+1)])\n",
    "    df_transformed_reduced[\"pChEMBL.Value\"]=y\n",
    "    df_transformed_reduced[\"Name\"]=Id\n",
    "    if not os.path.isdir(os.path.join(os.getcwd(),'STS-dataset\\\\fingerprints\\\\transformed--reduced')):\n",
    "        os.mkdir(os.path.join(os.getcwd(),'STS-dataset\\\\fingerprints\\\\transformed--reduced'))\n",
    "    df_transformed.to_csv(os.path.join(os.getcwd(),'STS-dataset\\\\fingerprints\\\\transformed--reduced\\\\{}--reduced.csv'.format(dfs[:-4])))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining the Transformed Files into one dataframe after having reduced the dimensions for each fingerprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 9/12 [01:43<01:23, 27.80s/it]"
     ]
    }
   ],
   "source": [
    "df_combined=None\n",
    "files_transformed=os.listdir(os.path.join(os.getcwd(),'STS-dataset\\\\fingerprints\\\\transformed--reduced'))##change the directory\n",
    "s=-1\n",
    "for dfs in tqdm(files_transformed):\n",
    "    s+=1\n",
    "    if s==0:\n",
    "        df_transformed=pd.read_csv(os.path.join(os.getcwd(),'STS-dataset\\\\fingerprints\\\\transformed--reduced\\\\{}.csv'.format(dfs[:-4])))\n",
    "        df_combined=df_transformed\n",
    "    else:\n",
    "        df_transformed=pd.read_csv(os.path.join(os.getcwd(),'STS-dataset\\\\fingerprints\\\\transformed--reduced\\\\{}.csv'.format(dfs[:-4])))\n",
    "        df_combined=df_combined.merge(df_transformed,how=\"inner\",on=\"Name\")\n",
    "df_combined.to_csv(os.path.join(os.getcwd(),\"df_new_data_dl.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined=pd.read_csv(os.path.join(os.getcwd(),\"df_new_data_dl.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Preprocessing of df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_combined.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.drop(['Unnamed: 0','Unnamed: 0_x','Unnamed: 0_y','Unnamed: 0_x.1',\n",
    "                 'Unnamed: 0_x.2','Unnamed: 0_x.3','Unnamed: 0_y.2','Unnamed: 0_y.3',\n",
    "                 'Unnamed: 0_x.4','Unnamed: 0_y.4','Unnamed: 0_x.5','Unnamed: 0_y.5','pChEMBL.Value_y.5',\n",
    "                 'pChEMBL.Value_x.5','pChEMBL.Value_y.4','pChEMBL.Value_x.4','pChEMBL.Value_y.3','pChEMBL.Value_x.3',\n",
    "                 'pChEMBL.Value_y.2','pChEMBL.Value_x.2','pChEMBL.Value_y.1','pChEMBL.Value_x.1','pChEMBL.Value_y'],1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df_combined.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined=df_combined.rename(columns={\"pChEMBL.Value_x\":\"pChEMBL.Value\"})\n",
    "best_ncompnents(df_combined,threshold=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,_,_,y=PCA_function(df_combined,13,visual=True) \n",
    "np.save(\"X.npy\",X)\n",
    "np.save(\"y.npy\",y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_learning_model():\n",
    "    \n",
    "    inputs = Input(shape=(13,))\n",
    "    a_1 = Dense(32,activation=\"relu\")(inputs)\n",
    "    a_2 = Dense(128,activation=\"relu\")(a_1)\n",
    "    output=Dense(1,activation=\"linear\")(a_2)\n",
    "    \n",
    "    model =Model(inputs=inputs, outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False),\n",
    "                 loss='mean_squared_error')\n",
    "    \n",
    "    print(model.summary())\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_val,y_train,y_val=train_test_split(X,y,test_size=0.1)\n",
    "checkpoint=ModelCheckpoint(r\"C:\\Users\\MMOHTASHIM\\Anaconda3\\libs\\IBM Capstone\",monitor='val_loss', save_best_only=False)\n",
    "earlystop=EarlyStopping(monitor='val_loss', min_delta=0, patience=2)\n",
    "model=deep_learning_model()\n",
    "X_train,X_val,y_train,y_val=train_test_split(X,y,test_size=0.1)\n",
    "history=model.fit(X_train,y_train,epochs=5,validation_data=(X_val,y_val),callbacks=[checkpoint,earlystop])\n",
    "ax=plt.subplot(111)\n",
    "ax.plot(history.history[\"loss\"],label=\"loss\")\n",
    "ax.plot(history.history[\"val_loss\"],label=\"validation_loss\")\n",
    "ax.set_title(\"Loss Over Epochs\")\n",
    "ax.set_xlabel(\"Epochs\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"DeepLearning-Model V1.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrambled vs Orginal Performance Benchmark on Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrambled_pair_prediction(Deep_learning_algorithm,X_reduced,y,trials):\n",
    "    style.use(\"classic\")\n",
    "    \n",
    "    \n",
    "    clf=Deep_learning_algorithm\n",
    "\n",
    "    r2_test_scrambled=[]\n",
    "    r2_test_orginal=[]\n",
    "    r2_train_orginal=[]\n",
    "    r2_train_scrambled=[]\n",
    "    y_train_combined=[]\n",
    "    y_train_scrambled_combined=[]\n",
    "    y_test_combined=[]\n",
    "    y_test_scrambled_combined=[]\n",
    "\n",
    "    \n",
    "    for i in range(5):##10 time shuffled\n",
    "        X_shuffled=np.copy(X_reduced)\n",
    "        y_shuffled=np.copy(y)\n",
    "        np.random.shuffle(X_shuffled)\n",
    "        np.random.shuffle(y_shuffled)\n",
    "        X_train_scrambled,X_test_scrambled,y_train_scrambled,y_test_scrambled=train_test_split(X_shuffled,y_shuffled,test_size=0.2)##80/20 indpedent split on scrambled data\n",
    "        \n",
    "        X_train,X_test,y_train,y_test=train_test_split(X_reduced,y,test_size=0.2)##80/20 indpedent split on orginal\n",
    "        y_test_scrambled_combined.append(clf.predict(X_test_scrambled))\n",
    "        y_test_combined.append(clf.predict(X_test))\n",
    "        y_train_combined.append(clf.predict(X_train))\n",
    "        y_train_scrambled_combined.append(clf.predict(X_train_scrambled))\n",
    "        for i in range(trials):##for each shuffled ,run this many times of this trial and collect the revelant for later plotting\n",
    "            r2_test_scrambled.append(r2_score(y_test_scrambled,y_pred_test_scrambled_trial))\n",
    "            r2_test_orginal.append(r2_score(y_test,y_pred_test_trial))\n",
    "\n",
    "\n",
    "            r2_train_scrambled.append(r2_score(y_train_scrambled,y_pred_train_scrambled_trial))\n",
    "            r2_train_orginal.append(r2_score(y_train,y_pred_train_trial))\n",
    "        \n",
    "    \n",
    "    fig=plt.figure()\n",
    "    fig.set_facecolor('white')\n",
    "    fig.set_size_inches(14,14)\n",
    "    ax1=fig.add_subplot(211)\n",
    "    ax2=fig.add_subplot(212)\n",
    "   \n",
    "        \n",
    "    ax1.grid(False)\n",
    "    ax2.grid(False)\n",
    "\n",
    "    \n",
    "    ax1.scatter(np.array(y_test_combined),np.array(y_pred_test).reshape(-1,),c='red',label='Orginal')\n",
    "    ax1.set_title('Predicted vs Orginal pChEMBL--Test Data')\n",
    "    \n",
    "    \n",
    "    ax1.scatter(np.array(y_test_scrambled_combined), np.array(y_pred_test_scrambled).reshape(-1,),c='blue',label='Scrambled')\n",
    "    \n",
    "    \n",
    "    \n",
    "    ax2.scatter(np.array(r2_test_scrambled),np.array(r2_test_orginal).reshape(-1,),c='red',label=\"test\")\n",
    "    ax2.set_title('R2 for Scrambled vs Orginal')\n",
    "    \n",
    "    ax2.scatter(np.array(r2_train_scrambled),np.array(r2_train_orginal).reshape(-1,),c='blue',label=\"train\")\n",
    "   \n",
    "    \n",
    "#     fig.text(0.02, 0.5, 'Predicted pChEMBL', ha='center',fontweight=\"bold\")\n",
    "#     fig.text(0.5,0.5,'R2-Orginal',ha='center',fontweight=\"bold\")\n",
    "    \n",
    "    ax1.legend(loc=4)\n",
    "    ax2.legend(loc=4)\n",
    "    ax1.set_xlabel(\"Experimental pChEMBL\",fontweight=\"bold\")\n",
    "    ax2.set_xlabel(\"R2-Scrambled\",fontweight=\"bold\")\n",
    "    \n",
    "    \n",
    "    ax1.set_ylabel(\"Predicted pChEMBL\",fontweight=\"bold\")\n",
    "    ax2.set_ylabel(\"R2-Orginal\",fontweight=\"bold\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.load(\"X.npy\")\n",
    "y=np.load(\"y.npy\")\n",
    "model = load_model('DeepLearning-Model V1.h5')\n",
    "scrambled_pair_prediction(model,X,y,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning Model Metric MSE and R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_learning_train():\n",
    "\n",
    "    r2_train=[]\n",
    "    r2_test=[]\n",
    "    mse_train=[]\n",
    "    mse_test=[]\n",
    "    \n",
    "    for i in range(3):\n",
    "\n",
    "        checkpoint=ModelCheckpoint(r\"C:\\Users\\MMOHTASHIM\\Anaconda3\\libs\\IBM Capstone\",monitor='val_loss', save_best_only=False)\n",
    "        earlystop=EarlyStopping(monitor='val_loss', min_delta=0, patience=2)\n",
    "        clf=deep_learning_model()\n",
    "        X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\n",
    "        clf.fit(X_train,y_train,epochs=20,validation_data=(X_val,y_val),callbacks=[checkpoint,earlystop])\n",
    "            \n",
    "        y_pred_train=clf.predict(X_train)\n",
    "        y_pred_test=clf.predict(X_test)##testing\n",
    "        \n",
    "        r2_train.append(r2_score(y_train,y_pred_train))\n",
    "        r2_test.append(r2_score(y_test,y_pred_test))\n",
    "        mse_train.append(mean_squared_error(y_train,y_pred_train))\n",
    "        mse_test.append(mean_squared_error(y_test,y_pred_test))\n",
    "        \n",
    "        \n",
    "    print(\"The mean R2 score for {} is {}--Train\".format(\"Deep Learning Model is\",np.mean(r2_train)))\n",
    "    print(\"The mean R2 score for {} is {}--Test\".format(\"Deep Learning Model is\",np.mean(r2_test)))\n",
    "    \n",
    "    print(\"The mean MSE score for {} is {}--Train\".format(\"Deep Learning Model is\",np.mean(mse_train)))\n",
    "    print(\"The mean MSE score for {} is {}--Test\".format(\"Deep Learning Model is\",np.mean(mse_test)))\n",
    "    \n",
    "    fig, axs = plt.subplots(2, 2)\n",
    "    fig.set_size_inches(11,8)\n",
    "    axs[0, 0].plot(list(range(trials)), r2_train,'tab:red')\n",
    "    axs[0, 0].set_title('R2--Score--Train--{}--trials'.format(trials))\n",
    "    axs[0, 0].set_ylabel(\"R2 Score\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    axs[0, 1].plot(list(range(trials)), r2_test, 'tab:orange')\n",
    "    axs[0, 1].set_title('R2--Score--Test--{}--trials'.format(trials))\n",
    "    axs[1, 0].plot(list(range(trials)), mse_train,'tab:blue')\n",
    "    \n",
    "    \n",
    "    axs[1, 0].set_title('MSE--Score--Train--{}--trials'.format(trials))\n",
    "    axs[1, 0].set_ylabel(\"MSE Score\")\n",
    "    axs[1, 0].set_xlabel(\"Trials\")\n",
    "    axs[1, 1].plot(list(range(trials)), mse_test,'tab:purple')\n",
    "    axs[1, 1].set_title('MSE--Score--Test--{}--trials'.format(trials))\n",
    "    axs[1, 1].set_xlabel(\"Trials\")\n",
    "    \n",
    "    \n",
    "    axs[0,0].set_ylim([0,1])\n",
    "    axs[0,1].set_ylim([0,1])\n",
    "    axs[1,0].set_ylim([0,2])\n",
    "    axs[1,1].set_ylim([0,2])\n",
    "    plt.grid(False)\n",
    "    \n",
    "    return (r2_train,r2_test,mse_train,mse_test)\n",
    "   \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'deep_learning_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-2eb3a8d3da2b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdeep_learning_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-cb09e1540546>\u001b[0m in \u001b[0;36mdeep_learning_train\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mcheckpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModelCheckpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"C:\\Users\\MMOHTASHIM\\Anaconda3\\libs\\IBM Capstone\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mearlystop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_delta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mclf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdeep_learning_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mearlystop\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'deep_learning_model' is not defined"
     ]
    }
   ],
   "source": [
    "deep_learning_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
